{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e37023d",
   "metadata": {},
   "source": [
    "## Model selection and baseline model\n",
    "\n",
    "### \n",
    "We need to choose appropriate models for this multi-class classification task. Starting with a simple baseline model (e.g., Logistic Regression, Naive Bayes) to establish a performance benchmark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27303ce5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df_processed.drop(columns=['role', 'index'])\n",
    "y = df_processed['role']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train Logistic Regression model\n",
    "logistic_regression_model = LogisticRegression(max_iter=1000)\n",
    "logistic_regression_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = logistic_regression_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision (weighted): {precision:.4f}\")\n",
    "print(f\"Recall (weighted): {recall:.4f}\")\n",
    "print(f\"F1-score (weighted): {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bd4317",
   "metadata": {},
   "source": [
    "## Improved model experimentation\n",
    "\n",
    "### \n",
    "Experiment with more advanced models (e.g., RandomForest, Gradient Boosting, or potentially neural networks for the text data) to try and improve upon the baseline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c58f06",
   "metadata": {},
   "source": [
    "Initialize, train, and evaluate a RandomForestClassifier model to compare its performance against the baseline Logistic Regression model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ce2ae5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a RandomForestClassifier model\n",
    "random_forest_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = random_forest_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf, average='weighted')\n",
    "recall_rf = recall_score(y_test, y_pred_rf, average='weighted')\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Random Forest Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy_rf:.4f}\")\n",
    "print(f\"Precision (weighted): {precision_rf:.4f}\")\n",
    "print(f\"Recall (weighted): {recall_rf:.4f}\")\n",
    "print(f\"F1-score (weighted): {f1_rf:.4f}\")\n",
    "\n",
    "# Compare with baseline Logistic Regression\n",
    "print(\"\\nBaseline Logistic Regression Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision (weighted): {precision:.4f}\")\n",
    "print(f\"Recall (weighted): {recall:.4f}\")\n",
    "print(f\"F1-score (weighted): {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f810925d",
   "metadata": {},
   "source": [
    "Initialize, train, and evaluate an XGBoost model, then print its performance metrics alongside the Random Forest metrics for comparison.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8af88d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the target variable\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Initialize an XGBClassifier model\n",
    "xgboost_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# Train the model with encoded labels\n",
    "xgboost_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Make predictions with encoded labels\n",
    "y_pred_xgb_encoded = xgboost_model.predict(X_test)\n",
    "\n",
    "# Decode the predictions back to original labels for evaluation\n",
    "y_pred_xgb = label_encoder.inverse_transform(y_pred_xgb_encoded)\n",
    "\n",
    "# Evaluate the model using original labels\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "precision_xgb = precision_score(y_test, y_pred_xgb, average='weighted')\n",
    "recall_xgb = recall_score(y_test, y_pred_xgb, average='weighted')\n",
    "f1_xgb = f1_score(y_test, y_pred_xgb, average='weighted')\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"XGBoost Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy_xgb:.4f}\")\n",
    "print(f\"Precision (weighted): {precision_xgb:.4f}\")\n",
    "print(f\"Recall (weighted): {recall_xgb:.4f}\")\n",
    "print(f\"F1-score (weighted): {f1_xgb:.4f}\")\n",
    "\n",
    "# Compare with RandomForest\n",
    "print(\"\\nRandom Forest Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy_rf:.4f}\")\n",
    "print(f\"Precision (weighted): {precision_rf:.4f}\")\n",
    "print(f\"Recall (weighted): {recall_rf:.4f}\")\n",
    "print(f\"F1-score (weighted): {f1_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed55299",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning \n",
    "\n",
    "### \n",
    "We first split the training data, define parameter grids for RandomForest and XGBoost, perform GridSearchCV for both models, print the best parameters, and evaluate the best models on the validation set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bcb2be",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Ensure y_train is available (it was defined in a previous step)\n",
    "if 'y_train' not in globals():\n",
    "    print(\"Error: y_train not found. Please ensure previous steps were executed.\")\n",
    "else:\n",
    "    # 1. Split the training data into a smaller training set and a validation set\n",
    "    X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    # 2. Define a parameter grid for hyperparameter tuning for the RandomForestClassifier\n",
    "    param_grid_rf = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5]\n",
    "    }\n",
    "\n",
    "    # 3. Use GridSearchCV with the RandomForestClassifier\n",
    "    grid_search_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=5, scoring='f1_weighted')\n",
    "    grid_search_rf.fit(X_train_split, y_train_split)\n",
    "\n",
    "    # 4. Print the best parameters found by GridSearchCV for the RandomForestClassifier\n",
    "    print(\"Best parameters for RandomForestClassifier:\")\n",
    "    print(grid_search_rf.best_params_)\n",
    "\n",
    "    # 5. Evaluate the best RandomForestClassifier model on the validation set\n",
    "    best_rf_model = grid_search_rf.best_estimator_\n",
    "    y_pred_rf_val = best_rf_model.predict(X_val)\n",
    "    f1_val_rf = f1_score(y_val, y_pred_rf_val, average='weighted')\n",
    "    print(f\"F1-weighted score on validation set (RandomForest): {f1_val_rf:.4f}\")\n",
    "\n",
    "    # 6. Define a parameter grid for hyperparameter tuning for the XGBClassifier\n",
    "    # Encode the split target variables for XGBoost\n",
    "    label_encoder_xgb = LabelEncoder()\n",
    "    y_train_encoded_split = label_encoder_xgb.fit_transform(y_train_split)\n",
    "    y_val_encoded = label_encoder_xgb.transform(y_val)\n",
    "\n",
    "    param_grid_xgb = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5],\n",
    "        'learning_rate': [0.01, 0.1]\n",
    "    }\n",
    "\n",
    "    # 7. Use GridSearchCV with the XGBClassifier\n",
    "    grid_search_xgb = GridSearchCV(XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss'), param_grid_xgb, cv=5, scoring='f1_weighted')\n",
    "    grid_search_xgb.fit(X_train_split, y_train_encoded_split)\n",
    "\n",
    "    # 8. Print the best parameters found by GridSearchCV for the XGBClassifier\n",
    "    print(\"\\nBest parameters for XGBClassifier:\")\n",
    "    print(grid_search_xgb.best_params_)\n",
    "\n",
    "    # 9. Make predictions on the encoded validation set using the best XGBClassifier model\n",
    "    best_xgb_model = grid_search_xgb.best_estimator_\n",
    "    y_pred_xgb_encoded_val = best_xgb_model.predict(X_val)\n",
    "\n",
    "    # 10. Decode the predictions back to original labels\n",
    "    y_pred_xgb_val = label_encoder_xgb.inverse_transform(y_pred_xgb_encoded_val)\n",
    "\n",
    "    # 11. Evaluate the best XGBClassifier model on the validation set\n",
    "    f1_val_xgb = f1_score(y_val, y_pred_xgb_val, average='weighted')\n",
    "    print(f\"F1-weighted score on validation set (XGBoost): {f1_val_xgb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a88524",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "### \n",
    "Evaluate the final chosen model using appropriate metrics for multi-class classification (e.g., accuracy, precision, recall, F1-score) on a separate test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8c8a26",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Select the best performing model based on validation F1 score\n",
    "# Compare f1_val_rf and f1_val_xgb\n",
    "if f1_val_xgb > f1_val_rf:\n",
    "    final_model = best_xgb_model\n",
    "    print(\"Selected XGBoost as the final model based on validation F1 score.\")\n",
    "    # Re-encode y_test for final evaluation with XGBoost if needed\n",
    "    y_test_encoded_final = label_encoder_xgb.transform(y_test)\n",
    "    y_pred_final = final_model.predict(X_test)\n",
    "    y_pred_final = label_encoder_xgb.inverse_transform(y_pred_final) # Decode predictions\n",
    "else:\n",
    "    final_model = best_rf_model\n",
    "    print(\"Selected RandomForest as the final model based on validation F1 score.\")\n",
    "    y_pred_final = final_model.predict(X_test)\n",
    "\n",
    "\n",
    "# Calculate evaluation metrics on the test set\n",
    "accuracy_test = accuracy_score(y_test, y_pred_final)\n",
    "precision_test = precision_score(y_test, y_pred_final, average='weighted')\n",
    "recall_test = recall_score(y_test, y_pred_final, average='weighted')\n",
    "f1_test = f1_score(y_test, y_pred_final, average='weighted')\n",
    "\n",
    "# Print the evaluation metrics for the final model on the test set\n",
    "print(\"\\nFinal Model Performance on Test Set:\")\n",
    "print(f\"Accuracy: {accuracy_test:.4f}\")\n",
    "print(f\"Precision (weighted): {precision_test:.4f}\")\n",
    "print(f\"Recall (weighted): {recall_test:.4f}\")\n",
    "print(f\"F1-score (weighted): {f1_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0531d714",
   "metadata": {},
   "source": [
    "Train the selected improved model (RandomForestClassifier) using cross-validation on the training data to get a more robust performance estimate and potentially tune hyperparameters if necessary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73325480",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define the model to be used for cross-validation (the random_forest_model instantiated in the previous step is suitable)\n",
    "# Use the same random_forest_model instance\n",
    "\n",
    "# Perform 5-fold cross-validation on the training data\n",
    "cv_scores = cross_val_score(random_forest_model, X_train, y_train, cv=5, scoring='f1_weighted')\n",
    "\n",
    "# Print the mean and standard deviation of the cross-validation F1 scores\n",
    "print(\"Cross-validation F1-weighted scores:\", cv_scores)\n",
    "print(f\"Mean CV F1-weighted score: {cv_scores.mean():.4f}\")\n",
    "print(f\"Standard deviation of CV F1-weighted scores: {cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6077f9",
   "metadata": {},
   "source": [
    "Evaluate the trained Random Forest model on the test set using accuracy, weighted precision, weighted recall, and weighted F1-score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84603480",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Make predictions on the test set using the trained random_forest_model\n",
    "y_pred_rf_test = random_forest_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy_test = accuracy_score(y_test, y_pred_rf_test)\n",
    "precision_test = precision_score(y_test, y_pred_rf_test, average='weighted')\n",
    "recall_test = recall_score(y_test, y_pred_rf_test, average='weighted')\n",
    "f1_test = f1_score(y_test, y_pred_rf_test, average='weighted')\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Random Forest Model Performance on Test Set:\")\n",
    "print(f\"Accuracy: {accuracy_test:.4f}\")\n",
    "print(f\"Precision (weighted): {precision_test:.4f}\")\n",
    "print(f\"Recall (weighted): {recall_test:.4f}\")\n",
    "print(f\"F1-score (weighted): {f1_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726241ff",
   "metadata": {},
   "source": [
    "## Analysis of failure modes and lessons learned\n",
    "\n",
    "### \n",
    "Analyze where the model performs poorly and try to understand the reasons. Document lessons learned throughout the process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7228b63e",
   "metadata": {},
   "source": [
    "\n",
    "This makes a classification report and confusion matrix to understand the model's performance on individual classes and identify patterns in misclassifications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c28227",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_test, y_pred_rf_test)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_rf_test, labels=random_forest_model.classes_)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=random_forest_model.classes_, yticklabels=random_forest_model.classes_)\n",
    "plt.xlabel('Predicted Role')\n",
    "plt.ylabel('True Role')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
